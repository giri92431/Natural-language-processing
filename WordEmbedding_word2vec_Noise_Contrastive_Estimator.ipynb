{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOWNLOAD_FILENAME = 'sampleText.zip'\n",
    "\n",
    "def maybe_download(url_path,expected_bytes):\n",
    "    if not os.path.exists(DOWNLOAD_FILENAME):\n",
    "        filename,_ = urllib.request.urlretrieve(url_path,DOWNLOAD_FILENAME)\n",
    "        \n",
    "    satatinfo =os.stat(DOWNLOAD_FILENAME)\n",
    "    if satatinfo.st_size == expected_bytes:\n",
    "        print('found and verified file :',url_path)\n",
    "        print('DOWNLOAD_FILE :',DOWNLOAD_FILENAME)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        rasie ('Faild to verify file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_words():\n",
    "    with zipfile.ZipFile(DOWNLOAD_FILENAME) as f:\n",
    "        firstFile =f.namelist()[0]\n",
    "        fileString =tf.compat.as_str(f.read(firstFile))\n",
    "        words = fileString.split()\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found and verified file : http://mattmahoney.net/dc/text8.zip\n",
      "DOWNLOAD_FILE : sampleText.zip\n"
     ]
    }
   ],
   "source": [
    "url_path ='http://mattmahoney.net/dc/text8.zip'\n",
    "FILESIZE = 31344016\n",
    "maybe_download(url_path,FILESIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17005207\n"
     ]
    }
   ],
   "source": [
    "vocabulary = read_words()\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataSet(words,n_words):\n",
    "    word_counts = [['unknown',-1]]\n",
    "    counter = collections.Counter(words)\n",
    "    word_counts.extend(counter.most_common( n_words - 1))\n",
    "    dictonary = dict()\n",
    "    for word ,_ in word_counts:\n",
    "        dictonary[word] = len(dictonary)\n",
    "        \n",
    "    word_indxes = list()\n",
    "    unknown_count = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if word in dictonary:\n",
    "            index = dictonary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "            unknown_count +=1\n",
    "        word_indxes.append(index)\n",
    "    word_counts[0][1] = unknown_count\n",
    "    \n",
    "    reversed_dictionary = dict(zip(dictonary.values(),dictonary.keys()))\n",
    "    return word_counts ,word_indxes ,dictonary ,reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 5000\n",
    "word_counts ,word_indxes ,dictonary ,reversed_dictionary = build_dataSet(vocabulary,vocabulary_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_batch(word_indxes,batch_size,num_skips,skip_window):\n",
    "    global global_index\n",
    "    \n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size),dtype = np.int32)\n",
    "    lables = np.ndarray(shape=(batch_size ,1) ,dtype=np.int32)\n",
    "    \n",
    "    span = 2 * skip_window + 1\n",
    "    \n",
    "    buffer = collections.deque(maxlen=span)\n",
    " \n",
    "    for _ in range(span):\n",
    "        buffer.append(word_indxes[global_index])\n",
    "        global_index = (global_index + 1) % len(word_indxes)\n",
    "    \n",
    "    for i in range(batch_size //num_skips):\n",
    "        target = skip_window\n",
    "        targets_to_avoid = [skip_window]\n",
    "        \n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0,span - 1)\n",
    "            \n",
    "            targets_to_avoid.append(target)\n",
    "            \n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            lables[i * num_skips + j, 0] = buffer[target]\n",
    "            \n",
    "        buffer.append(word_indxes[global_index])\n",
    "        global_index = (global_index + 1) % len(word_indxes)\n",
    "        \n",
    "    global_index = (global_index + len(word_indxes) -span )% len(word_indxes)\n",
    "        \n",
    "    return batch,lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch ,labels = generator_batch(word_indxes ,10,2,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ready to set up nural network using tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset global index becasue we updated while testign the batch code\n",
    "global_index = 0\n",
    "valid_size = 16\n",
    "valid_window = 100\n",
    "valid_example = np.random.choice(valid_window,valid_size,replace= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_samples = 64\n",
    "# number of corrupted sample pairs to feed into the NCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 50\n",
    "skip_window = 2 \n",
    "num_skips = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "train_inputs = tf.placeholder(tf.int32 ,shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32,shape=[batch_size ,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate_dataset = tf.constant(valid_example ,dtype =tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(\n",
    "                tf.random_uniform([vocabulary_size,embedding_size], -1.0, 1.0))\n",
    "embed = tf.nn.embedding_lookup(embeddings,train_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nce_weights = tf.Variable(\n",
    "    tf.truncated_normal([vocabulary_size ,embedding_size],stddev=1.0 / math.sqrt(embedding_size))\n",
    ")\n",
    "nce_biase = tf.Variable(tf.zeros([vocabulary_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss =tf.reduce_mean(tf.nn.nce_loss(\n",
    "    weights = nce_weights,\n",
    "    biases  = nce_biase,\n",
    "    labels  = train_labels,\n",
    "    inputs  = embed,\n",
    "    num_sampled = num_samples,\n",
    "    num_classes = vocabulary_size\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer =tf.train.GradientDescentOptimizer(0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l2_norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings),1,keep_dims =True))\n",
    "\n",
    "normalized_embeddings = embeddings /l2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_embeddings = tf.nn.embedding_lookup(normalized_embeddings,validate_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similarity = tf.matmul(validation_embeddings,normalized_embeddings,transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## noise contrastive estimator to measssure loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps =20001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss at step  0  :  200.609985352\n",
      "nerest to no ban , peer , planet , few , equally , foods , worked , pope ,\n",
      "nerest to on itself , market , farm , large , artillery , identified , eye , bell ,\n",
      "nerest to united a , chad , possibility , diseases , chapters , defend , excellent , dedicated ,\n",
      "nerest to known over , ended , prisoners , mm , indians , advocate , children , publicly ,\n",
      "nerest to but benjamin , missile , explained , nine , telecommunications , raise , americans , letters ,\n",
      "nerest to other diet , planet , transferred , napoleon , rival , mainland , giving , tiny ,\n",
      "nerest to he detroit , originated , extensive , prominent , colleges , employees , franklin , isle ,\n",
      "nerest to has adults , none , gain , located , pattern , differences , iraq , contemporary ,\n",
      "nerest to many nonetheless , intel , requirement , inventor , o , formal , markets , siege ,\n",
      "nerest to seven seat , beam , account , graphic , kosovo , trace , contest , historically ,\n",
      "nerest to will merely , anderson , views , carbon , huge , listed , scored , types ,\n",
      "nerest to an plants , triple , meters , legacy , whose , concrete , programming , gun ,\n",
      "nerest to with values , ascii , processes , del , true , concern , worth , force ,\n",
      "nerest to history electoral , duties , societies , captured , debt , compounds , thanks , constitution ,\n",
      "nerest to this e , newer , local , prevent , beings , aluminium , hockey , hotel ,\n",
      "nerest to his caesar , founder , oriented , learn , course , experts , independent , serving ,\n",
      "\n",
      "\n",
      "average loss at step  2000  :  68.2348003688\n",
      "average loss at step  4000  :  26.0889133562\n",
      "average loss at step  6000  :  17.0358624103\n",
      "average loss at step  8000  :  12.2907410667\n",
      "average loss at step  10000  :  9.72012792706\n",
      "nerest to no few , than , america , which , not , those , still , now ,\n",
      "nerest to on and , large , like , itself , music , what , market , first ,\n",
      "nerest to united a , agave , an , language , little , has , were , three ,\n",
      "nerest to known over , first , most , their , but , philosophy , into , children ,\n",
      "nerest to but nine , philosophy , state , year , or , them , eight , president ,\n",
      "nerest to other such , where , four , six , most , zero , he , small ,\n",
      "nerest to he lincoln , seven , five , after , most , west , world , became ,\n",
      "nerest to has that , war , six , would , it , nine , five , eight ,\n",
      "nerest to many had , time , that , are , has , new , good , to ,\n",
      "nerest to seven lincoln , zero , he , area , be , one , after , much ,\n",
      "nerest to will however , not , lincoln , most , zero , but , also , gore ,\n",
      "nerest to an would , out , united , hiv , when , however , at , their ,\n",
      "nerest to with their , way , early , south , apollo , further , seven , for ,\n",
      "nerest to history lincoln , between , km , it , usually , often , force , such ,\n",
      "nerest to this e , at , from , if , four , play , zero , war ,\n",
      "nerest to his ocean , before , made , such , alaska , state , east , seven ,\n",
      "\n",
      "\n",
      "average loss at step  12000  :  8.25798319209\n",
      "average loss at step  14000  :  7.15146651924\n",
      "average loss at step  16000  :  6.3535241102\n",
      "average loss at step  18000  :  5.83010849476\n",
      "average loss at step  20000  :  5.88376504934\n",
      "nerest to no few , america , still , those , than , worked , personal , gore ,\n",
      "nerest to on large , alternate , and , market , music , like , itself , bell ,\n",
      "nerest to united agave , a , little , language , period , should , anime , described ,\n",
      "nerest to known over , children , most , first , ended , philosophy , mm , court ,\n",
      "nerest to but philosophy , nine , letters , state , year , president , region , college ,\n",
      "nerest to other such , humans , small , where , required , germany , napoleon , important ,\n",
      "nerest to he west , real , lincoln , high , around , low , after , extensive ,\n",
      "nerest to has none , located , asia , that , war , book , would , japan ,\n",
      "nerest to many together , players , o , had , good , time , culture , become ,\n",
      "nerest to seven lincoln , area , released , family , much , zero , one , humans ,\n",
      "nerest to will lincoln , however , civil , gore , best , carbon , present , both ,\n",
      "nerest to an hiv , out , plants , united , would , arts , strong , g ,\n",
      "nerest to with way , further , early , values , their , south , worth , ascii ,\n",
      "nerest to history lincoln , km , between , force , compounds , chain , usually , region ,\n",
      "nerest to this e , least , local , play , sound , created , aluminium , if ,\n",
      "nerest to his independent , founder , course , produced , ocean , india , significant , east ,\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as session:\n",
    "    init.run()\n",
    "    \n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs ,batch_labels = generator_batch(word_indxes, batch_size,num_skips,skip_window)\n",
    "        \n",
    "        feed_dict = {train_inputs :batch_inputs, train_labels:batch_labels}\n",
    "        \n",
    "        _,loss_val = session.run([optimizer,loss] , feed_dict =feed_dict)\n",
    "        average_loss += loss_val\n",
    "        \n",
    "        if step % 2000 ==0:\n",
    "            if step >0:\n",
    "                average_loss /=2000\n",
    "            \n",
    "            print('average loss at step ',step,\" : \",average_loss)\n",
    "            average_loss = 0\n",
    "            \n",
    "        if step % 10000 ==0 :\n",
    "            sim = similarity.eval()\n",
    "            \n",
    "            for i in range(valid_size):\n",
    "                valid_word = reversed_dictionary[valid_example[i]]\n",
    "                top_k =8\n",
    "                \n",
    "                nearest = (-sim[i,:]).argsort()[1:top_k+1]\n",
    "                log_str = \"nerest to \" + valid_word\n",
    "                \n",
    "                for k in range(top_k):\n",
    "                    close_word = reversed_dictionary[nearest[k]]\n",
    "                    log_str = '%s %s ,' % (log_str,close_word)\n",
    "                print(log_str)\n",
    "            print(\"\\n\")\n",
    "    final_embedding = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_words_embeddings(embeddings,word_counts):\n",
    "    np.save('embeddings.npy',embeddings)\n",
    "    \n",
    "    words=[x[0] for x in word_counts]\n",
    "    np.save('words.npy',words)\n",
    "\n",
    "save_words_embeddings(final_embeddings,data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu]",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
