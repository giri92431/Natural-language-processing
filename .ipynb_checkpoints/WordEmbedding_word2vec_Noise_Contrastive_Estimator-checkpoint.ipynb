{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOWNLOAD_FILENAME = 'sampleText.zip'\n",
    "\n",
    "def maybe_download(url_path,expected_bytes):\n",
    "    if not os.path.exists(DOWNLOAD_FILENAME):\n",
    "        filename,_ = urllib.request.urlretrieve(url_path,DOWNLOAD_FILENAME)\n",
    "        \n",
    "    satatinfo =os.stat(DOWNLOAD_FILENAME)\n",
    "    if satatinfo.st_size == expected_bytes:\n",
    "        print('found and verified file :',url_path)\n",
    "        print('DOWNLOAD_FILE :',DOWNLOAD_FILENAME)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        rasie ('Faild to verify file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_words():\n",
    "    with zipfile.ZipFile(DOWNLOAD_FILENAME) as f:\n",
    "        firstFile =f.namelist()[0]\n",
    "        fileString =tf.compat.as_str(f.read(firstFile))\n",
    "        words = fileString.split()\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found and verified file : http://mattmahoney.net/dc/text8.zip\n",
      "DOWNLOAD_FILE : sampleText.zip\n"
     ]
    }
   ],
   "source": [
    "url_path ='http://mattmahoney.net/dc/text8.zip'\n",
    "FILESIZE = 31344016\n",
    "maybe_download(url_path,FILESIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17005207\n"
     ]
    }
   ],
   "source": [
    "vocabulary = read_words()\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataSet(words,n_words):\n",
    "    word_counts = [['unknown',-1]]\n",
    "    counter = collections.Counter(words)\n",
    "    word_counts.extend(counter.most_common( n_words - 1))\n",
    "    dictonary = dict()\n",
    "    for word ,_ in word_counts:\n",
    "        dictonary[word] = len(dictonary)\n",
    "        \n",
    "    word_indxes = list()\n",
    "    unknown_count = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if word in dictonary:\n",
    "            index = dictonary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "            unknown_count +=1\n",
    "        word_indxes.append(index)\n",
    "    word_counts[0][1] = unknown_count\n",
    "    \n",
    "    reversed_dictionary = dict(zip(dictonary.values(),dictonary.keys()))\n",
    "    return word_counts ,word_indxes ,dictonary ,reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 5000\n",
    "word_counts ,word_indxes ,dictonary ,reversed_dictionary = build_dataSet(vocabulary,vocabulary_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_batch(word_indxes,batch_size,num_skips,skip_window):\n",
    "    global global_index\n",
    "    \n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size),dtype = np.int32)\n",
    "    lables = np.ndarray(shape=(batch_size ,1) ,dtype=np.int32)\n",
    "    \n",
    "    span = 2 * skip_window + 1\n",
    "    \n",
    "    buffer = collections.deque(maxlen=span)\n",
    " \n",
    "    for _ in range(span):\n",
    "        buffer.append(word_indxes[global_index])\n",
    "        global_index = (global_index + 1) % len(word_indxes)\n",
    "    \n",
    "    for i in range(batch_size //num_skips):\n",
    "        target = skip_window\n",
    "        targets_to_avoid = [skip_window]\n",
    "        \n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0,span - 1)\n",
    "            \n",
    "            targets_to_avoid.append(target)\n",
    "            \n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            lables[i * num_skips + j, 0] = buffer[target]\n",
    "            \n",
    "        buffer.append(word_indxes[global_index])\n",
    "        global_index = (global_index + 1) % len(word_indxes)\n",
    "        \n",
    "    global_index = (global_index + len(word_indxes) -span )% len(word_indxes)\n",
    "        \n",
    "    return batch,lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  12],\n",
       "       [ 128],\n",
       "       [   6],\n",
       "       [ 195],\n",
       "       [ 742],\n",
       "       [  59],\n",
       "       [ 477],\n",
       "       [ 195],\n",
       "       [   0],\n",
       "       [3133]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch ,labels = generator_batch(word_indxes ,10,2,5)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ready to set up nural network using tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset global index becasue we updated while testign the batch code\n",
    "global_index = 0\n",
    "valid_size = 16\n",
    "valid_window = 100\n",
    "valid_example = np.random.choice(valid_window,valid_size,replace= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 50\n",
    "skip_window = 2 \n",
    "num_skips = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "train_inputs = tf.placeholder(tf.int32 ,shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32,shape=[batch_size ,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate_dataset = tf.constant(valid_example ,dtype =tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(\n",
    "                tf.random_uniform([vocabulary_size,embedding_size], -1.0, 1.0))\n",
    "embed = tf.nn.embedding_lookup(embeddings,train_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = tf.Variable(tf.truncated_normal([vocabulary_size , embedding_size],\n",
    "                                          stddev=1.0 / math.sqrt(embedding_size)))\n",
    "\n",
    "baises = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "hidden_out = tf.matmul(embed,tf.transpose(weights)) + baises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_one_hot = tf.one_hot(train_labels,vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss =tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hidden_out,labels=train_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer =tf.train.GradientDescentOptimizer(0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings),1,keep_dims =True))\n",
    "\n",
    "normalized_embeddings = embeddings /l2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_embeddings = tf.nn.embedding_lookup(normalized_embeddings,validate_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similarity = tf.matmul(validation_embeddings,normalized_embeddings,transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps =20001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss at step  0  :  8.62072563171\n",
      "[4719 1988 3771 3696 2257 4696 2840  171]\n",
      "[3625 2255 3558 2952 4348 1925 4686   48]\n",
      "[4036  757 1827 1724 1576 1334 2829  338]\n",
      "[2468 3137  913  387 2196  443 3115  920]\n",
      "[1384  720  520 4874 1158 1078 3011  324]\n",
      "[3425 2509  922 1895 3833 2777 4938 1763]\n",
      "[2541 4704 4375  920  286 2473 2815 3388]\n",
      "[3418 1281 3868 3460 2444  229 4924 1431]\n",
      "[4712 2721 4197  668 4928  501  600 3845]\n",
      "[3245 1977 1725 3848  793 1298 4641   45]\n",
      "[ 561  969  522 2841 4194 4375 3716 2683]\n",
      "[4009 3247 1339 4419 2308  871 4201 3872]\n",
      "[2137 2140 4452 2959 3247 1275  167 3559]\n",
      "[4535 1439 2935  298 3492 3827 3665 3592]\n",
      "[4134 1110 1653 1175 2710  557  593 1613]\n",
      "[ 426 2094 4412 4287 1756 1870 3267 4098]\n",
      "\n",
      "\n",
      "average loss at step  2000  :  6.9345065676\n",
      "average loss at step  4000  :  6.16278499198\n",
      "average loss at step  6000  :  6.05716351962\n",
      "average loss at step  8000  :  5.93226767993\n",
      "average loss at step  10000  :  5.71133075821\n",
      "[4719 1988 3771 3696 2257 4696 4605 2840]\n",
      "[3625 2255 3558 2952 1925 4686   48 4348]\n",
      "[1827  757 4036 2730 4660  338 2819 2775]\n",
      "[2468 3137 2196  387  443  858 1575 2558]\n",
      "[ 520  720 1384 4874 1078 3011 1158  324]\n",
      "[3425  922 1895 2509 4938 3833 3726 2777]\n",
      "[2541 4704 2815  286 2473 4375 2143 3388]\n",
      "[1281 3418 3868   18 1431 4423 2444  229]\n",
      "[2721 4197 4712  668 4928  501  600 3526]\n",
      "[3848 3245   22 1977  793 4918 4641 1725]\n",
      "[ 561  969 2841  522 4194 3716 4375 4152]\n",
      "[4009 3247  871 2308 4201  114 4419 1339]\n",
      "[2140 2137 4452 2959 1275  167 3247 4247]\n",
      "[4535 1439  298 2935 3492 3827 3665 3592]\n",
      "[4134 1110 1653 1175 2710  557 2567  593]\n",
      "[2094 4041 4188 3267  426  706 1756 4098]\n",
      "\n",
      "\n",
      "average loss at step  12000  :  5.76368791735\n",
      "average loss at step  14000  :  5.60461584139\n",
      "average loss at step  16000  :  5.5059720304\n",
      "average loss at step  18000  :  5.45614368218\n",
      "average loss at step  20000  :  5.69563402128\n",
      "[4719 1988 3771 3696 2257 4696 4605 2840]\n",
      "[3558 3625 2255 1925 2952 4686   48 4348]\n",
      "[1827  757  338 2730 4660 4036 2819 2775]\n",
      "[2468 2196 3137  387  443 1575  858 2558]\n",
      "[ 520  720 1384 4874 1078  195  324 4174]\n",
      "[3425  922 1895 4938 3726 2509 3833 2777]\n",
      "[2541 2815 4704  286 2143 2473 4375 3388]\n",
      "[1281   18 3418 3868 2444 1431 4423 4015]\n",
      "[4197 2721 4712  668 4928  600 3526  501]\n",
      "[  22 3848 3245 1977 4918  793   13 4641]\n",
      "[ 561  969 2841  522 4194 3716 4375 4152]\n",
      "[4009 3247  114  871 4201 2308 1269 4419]\n",
      "[2140 4452 2137 2959 1275  167 3247 4247]\n",
      "[4535 1439  298 2935 3492 3827   36 3665]\n",
      "[4134 1110 1653 1175 2710  557 2567  593]\n",
      "[2094 4041  152 4188 3064  264  706  789]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as session:\n",
    "    init.run()\n",
    "    \n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs ,batch_labels = generator_batch(word_indxes, batch_size,num_skips,skip_window)\n",
    "        \n",
    "        feed_dict = {train_inputs :batch_inputs, train_labels:batch_labels}\n",
    "        \n",
    "        _,loss_val = session.run([optimizer,loss] , feed_dict =feed_dict)\n",
    "        average_loss += loss_val\n",
    "        \n",
    "        if step % 2000 ==0:\n",
    "            if step >0:\n",
    "                average_loss /=2000\n",
    "            \n",
    "            print('average loss at step ',step,\" : \",average_loss)\n",
    "            average_loss = 0\n",
    "            \n",
    "        if step % 10000 ==0 :\n",
    "            sim = similarity.eval()\n",
    "            \n",
    "            for i in range(valid_size):\n",
    "                valid_word = reversed_dictionary[valid_example[i]]\n",
    "                top_k =8\n",
    "                nearest = (-sim[i,:]).argsort()[1:top_k+1]\n",
    "                print(nearest)\n",
    "                log_str = \"nerest to \" + valid_word\n",
    "                \n",
    "                for k in range(top_k):\n",
    "                    close_word = reversed_dictionary[nearest[k]]\n",
    "                    log_str = '%s %s ,' % (log_str,close_word)\n",
    "               \n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## noise contrastive estimator to measssure loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu]",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
