{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOWNLOAD_FILENAME = 'sampleText.zip'\n",
    "\n",
    "def maybe_download(url_path,expected_bytes):\n",
    "    if not os.path.exists(DOWNLOAD_FILENAME):\n",
    "        filename,_ = urllib.request.urlretrieve(url_path,DOWNLOAD_FILENAME)\n",
    "        \n",
    "    satatinfo =os.stat(DOWNLOAD_FILENAME)\n",
    "    if satatinfo.st_size == expected_bytes:\n",
    "        print('found and verified file :',url_path)\n",
    "        print('DOWNLOAD_FILE :',DOWNLOAD_FILENAME)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        rasie ('Faild to verify file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_words():\n",
    "    with zipfile.ZipFile(DOWNLOAD_FILENAME) as f:\n",
    "        firstFile =f.namelist()[0]\n",
    "        fileString =tf.compat.as_str(f.read(firstFile))\n",
    "        words = fileString.split()\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found and verified file : http://mattmahoney.net/dc/text8.zip\n",
      "DOWNLOAD_FILE : sampleText.zip\n"
     ]
    }
   ],
   "source": [
    "url_path ='http://mattmahoney.net/dc/text8.zip'\n",
    "FILESIZE = 31344016\n",
    "maybe_download(url_path,FILESIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17005207\n"
     ]
    }
   ],
   "source": [
    "vocabulary = read_words()\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataSet(words,n_words):\n",
    "    word_counts = [['unknown',-1]]\n",
    "    counter = collections.Counter(words)\n",
    "    word_counts.extend(counter.most_common( n_words - 1))\n",
    "    dictonary = dict()\n",
    "    for word ,_ in word_counts:\n",
    "        dictonary[word] = len(dictonary)\n",
    "        \n",
    "    word_indxes = list()\n",
    "    unknown_count = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if word in dictonary:\n",
    "            index = dictonary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "            unknown_count +=1\n",
    "        word_indxes.append(index)\n",
    "    word_counts[0][1] = unknown_count\n",
    "    \n",
    "    reversed_dictionary = dict(zip(dictonary.values(),dictonary.keys()))\n",
    "    return word_counts ,word_indxes ,dictonary ,reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 5000\n",
    "word_counts ,word_indxes ,dictonary ,reversed_dictionary = build_dataSet(vocabulary,vocabulary_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_batch(word_indxes,batch_size,num_skips,skip_window):\n",
    "    global global_index\n",
    "    \n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size),dtype = np.int32)\n",
    "    lables = np.ndarray(shape=(batch_size ,1) ,dtype=np.int32)\n",
    "    \n",
    "    span = 2 * skip_window + 1\n",
    "    \n",
    "    buffer = collections.deque(maxlen=span)\n",
    " \n",
    "    for _ in range(span):\n",
    "        buffer.append(word_indxes[global_index])\n",
    "        global_index = (global_index + 1) % len(word_indxes)\n",
    "    \n",
    "    for i in range(batch_size //num_skips):\n",
    "        target = skip_window\n",
    "        targets_to_avoid = [skip_window]\n",
    "        \n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0,span - 1)\n",
    "            \n",
    "            targets_to_avoid.append(target)\n",
    "            \n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            lables[i * num_skips + j, 0] = buffer[target]\n",
    "            \n",
    "        buffer.append(word_indxes[global_index])\n",
    "        global_index = (global_index + 1) % len(word_indxes)\n",
    "        \n",
    "    global_index = (global_index + len(word_indxes) -span )% len(word_indxes)\n",
    "        \n",
    "    return batch,lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[156],\n",
       "       [  0],\n",
       "       [128],\n",
       "       [  2],\n",
       "       [  2],\n",
       "       [ 12],\n",
       "       [195],\n",
       "       [477],\n",
       "       [ 46],\n",
       "       [195]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch ,labels = generator_batch(word_indxes ,10,2,5)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ready to set up nural network using tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset global index becasue we updated while testign the batch code\n",
    "global_index = 0\n",
    "valid_size = 16\n",
    "valid_window = 100\n",
    "valid_example = np.random.choice(valid_window,valid_size,replace= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 50\n",
    "skip_window = 2 \n",
    "num_skips = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "train_inputs = tf.placeholder(tf.int32 ,shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32,shape=[batch_size ,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate_dataset = tf.constant(valid_example ,dtype =tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(\n",
    "                tf.random_uniform([vocabulary_size,embedding_size], -1.0, 1.0))\n",
    "embed = tf.nn.embedding_lookup(embeddings,train_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = tf.Variable(tf.truncated_normal([vocabulary_size , embedding_size],\n",
    "                                          stddev=1.0 / math.sqrt(embedding_size)))\n",
    "\n",
    "baises = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "hidden_out = tf.matmul(embed,tf.transpose(weights)) + baises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_one_hot = tf.one_hot(train_labels,vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss =tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hidden_out,labels=train_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer =tf.train.GradientDescentOptimizer(0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l2_norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings),1,keep_dims =True))\n",
    "\n",
    "normalized_embeddings = embeddings /l2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_embeddings = tf.nn.embedding_lookup(normalized_embeddings,validate_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similarity = tf.matmul(validation_embeddings,normalized_embeddings,transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps =20001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss at step  0  :  8.60095024109\n",
      "[1290  569   58 3674 2169 3316 3799 2023]\n",
      "[2923 3117 2979  860 3413 2945 2093  284]\n",
      "[1899 3716 3043 1551 2507 2751 4246 1427]\n",
      "[ 332  317  413  109 4568 1093  423 4593]\n",
      "[3224 4577 3386 1705  336  173 2178  197]\n",
      "[3486 2117 3316 1573  939 1607 2627 3709]\n",
      "[ 708  797  915 1835 3283 1602 4489 1059]\n",
      "[3352  945 2701 3301 4303 3932 2934 4473]\n",
      "[2150 3137 1420  847 1625 1697 4461 1415]\n",
      "[3923 1907 1131 4973 1426 3050 3595 1817]\n",
      "[4269 4393 2343 1830  290 3736 3330 4860]\n",
      "[  40 2856 3603 1405 3786  435 1722  475]\n",
      "[1063 3263 1789 4272 1118 3851 4247 1234]\n",
      "[4943  748  151 3811 3187 1208 3917    6]\n",
      "[1311 4602  921 4801 2157 4368 3466  993]\n",
      "[   6 4305  687  624 3115 2890 4090 3641]\n",
      "\n",
      "\n",
      "average loss at step  2000  :  6.95636038339\n",
      "average loss at step  4000  :  6.16703868908\n",
      "average loss at step  6000  :  6.06072191095\n",
      "average loss at step  8000  :  5.93096885377\n",
      "average loss at step  10000  :  5.70816995525\n",
      "[1290 1874 4942   58  569 4527 3171 3674]\n",
      "[  21 3117  860 2945 2093 3413 2735 3897]\n",
      "[1899 3716 1551 3043 2751 1427 2507 1287]\n",
      "[ 332  317 4865  413 1093  423 4568 4492]\n",
      "[3224 4577  173 1705  336 3386 2178  349]\n",
      "[3486 3316 2117 1573 1607  976 4376 2627]\n",
      "[ 708  797 1835  915 3283 1602 1059 4489]\n",
      "[3352  945 4303 3932 3301 2934 4473 2163]\n",
      "[2150 3137 1420  847 1625 1697 4461 1415]\n",
      "[3923 1907 1426 1131 4973 1817 4003 4333]\n",
      "[4269 4393 2343 1830 2450 4860  290 2357]\n",
      "[  40 2856 3603 1405 3786 1722  435  475]\n",
      "[1063 3263 1789 4272 3851 4247 1118 1234]\n",
      "[4943  748  151 3811    6 1208 3187 1426]\n",
      "[ 921 1311 3466  993 4801  702 2083 4602]\n",
      "[   6  624 4305 2890 3115  687 4090 3641]\n",
      "\n",
      "\n",
      "average loss at step  12000  :  5.76742463434\n",
      "average loss at step  14000  :  5.60302816969\n",
      "average loss at step  16000  :  5.50703478926\n",
      "average loss at step  18000  :  5.45655371857\n",
      "average loss at step  20000  :  5.71354653645\n",
      "[   1 1874 4942 1290 4482 4527   58  569]\n",
      "[  21 3117   22  860 2093 2945 3897 2735]\n",
      "[1899 3716 3043 1551 1427 2751 2507  848]\n",
      "[ 332  317 4865 1093  423 4492 3404  413]\n",
      "[3224 4577  173  336 1705 3386 2178  349]\n",
      "[3486 3316 2117 1573 1607  976 4376 2627]\n",
      "[ 797  708 1835  915 3283   29 1059 4489]\n",
      "[3352  945 2934 3932 2163 4303 4473 4210]\n",
      "[2150 3137 1420  847 1625 1697 4461 1415]\n",
      "[3923 1426 1907 1131 4973 4333 4003 1817]\n",
      "[4269 4393 2343    9 1830 2450 2357 4860]\n",
      "[  40 2856 3603 3786 1722 1405  435  475]\n",
      "[1063 1789 3263 3851 4272 4247 1234 1118]\n",
      "[4943  151  748 3811 1208 1426  147 3187]\n",
      "[ 921 3466  993 1311 3330 3261 2083 3885]\n",
      "[   6  624 2890 3115 4305  687 4090  321]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as session:\n",
    "    init.run()\n",
    "    \n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs ,batch_labels = generator_batch(word_indxes, batch_size,num_skips,skip_window)\n",
    "        \n",
    "        feed_dict = {train_inputs :batch_inputs, train_labels:batch_labels}\n",
    "        \n",
    "        _,loss_val = session.run([optimizer,loss] , feed_dict =feed_dict)\n",
    "        average_loss += loss_val\n",
    "        \n",
    "        if step % 2000 ==0:\n",
    "            if step >0:\n",
    "                average_loss /=2000\n",
    "            \n",
    "            print('average loss at step ',step,\" : \",average_loss)\n",
    "            average_loss = 0\n",
    "            \n",
    "        if step % 10000 ==0 :\n",
    "            sim = similarity.eval()\n",
    "            \n",
    "            for i in range(valid_size):\n",
    "                valid_word = reversed_dictionary[valid_example[i]]\n",
    "                top_k =8\n",
    "                nearest = (-sim[i,:]).argsort()[1:top_k+1]\n",
    "                print(nearest)\n",
    "                log_str = \"nerest to \" + valid_word\n",
    "                \n",
    "                for k in range(top_k):\n",
    "                    close_word = reversed_dictionary[nearest[k]]\n",
    "                    log_str = '%s %s ,' % (log_str,close_word)\n",
    "               \n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu]",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
